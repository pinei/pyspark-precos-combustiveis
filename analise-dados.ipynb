{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83e89c6f-fd86-4a00-a408-7b7408fb996a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Coleta de dados\n",
    "\n",
    "Esse projeto em PySpark faz uma análise de dados de preços de combustíveis da ANP.\n",
    "\n",
    "O trabalho faz uso de dados publicados pela ANP (Agência Nacional de Petróleo)\n",
    "\n",
    "> Em cumprimento às determinações da Lei do Petróleo (Lei nº 9478/1997, artigo 8º), a ANP acompanha os preços praticados por revendedores de combustíveis automotivos e de gás liquefeito de petróleo envasilhado em botijões de 13 quilos (GLP P13), por meio de uma pesquisa semanal de preços realizada por empresa contratada.\n",
    "\n",
    "- [Série Histórica de Preços de Combustíveis e de GLP](https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/serie-historica-de-precos-de-combustiveis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7692d95-c2b6-4aa7-90a8-7c31e0f7b1d6",
   "metadata": {},
   "source": [
    "Coletamos a série histórica de \"Combustíveis automotivos\" que vai de 2004 a 2023. São 39 arquivos CSV totalizando aproximadamente 3.7 GB.\n",
    "\n",
    "![Amostra dos dados](./assets/amostra_planilha_revenda.png)\n",
    "\n",
    "- [Metadados em PDF](https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos/shpc/metadados-serie-historica-precos-combustiveis-1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b34829c-90ef-4e35-b5b2-2479a566f0fe",
   "metadata": {},
   "source": [
    "Temos as seguintes colunas nos arquivos CSV, em conformidade com a documentação de metadados:\n",
    "\n",
    "| Coluna            | Tipo    | Comentário                                                                           |\n",
    "| ----------------- | ------- | ------------------------------------------------------------------------------------ |\n",
    "| Regiao            | Texto   | Sigla da região do país (ex: S, N, SE)                                               |\n",
    "| Estado            | Texto   | Sigla de unidade federativa (ex: RJ, SP, MG)                                         |\n",
    "| Municipio         | Texto   | Nome de município                                                                    |\n",
    "| Revenda           | Texto   | Razão social do revendedor de combustível                                            |\n",
    "| CNPJ da Revenda   | Texto   | CNPJ do revendedor                                                                   |\n",
    "| Nome da Rua       | Texto   | Logradouro                                                                           |\n",
    "| Complemento       | Texto   | Logradouro                                                                           |\n",
    "| Bairro            | Texto   | Logradouro                                                                           |\n",
    "| CEP               | Texto   | Código de Endereçamento Postal                                                       |\n",
    "| Produto           | Texto   | Produto combustível (ex: GASOLINA, ETANOL, DIESEL)                                   |\n",
    "| Data da Coleta    | Data    | Data da pesquisa de preço (formato dd/mm/aaaa)                                       |\n",
    "| Valor de Venda    | Decimal | Valor de venda da unidade de combustível (4 casas decimais, vírgula como separador)  |\n",
    "| Valor de Compra   | Decimal | Valor de compra da unidade de combustível (4 casas decimais, virgula como separador) |\n",
    "| Unidade de Medida | Texto   | Unidade ao qual os valores de compra e venda se referem (ex: R$ / litro)             |\n",
    "| Bandeira          | Texto   | Nome de marca do posto de revenda (ex: IPIRANGA, BRANCA, COSAN, etc.)                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77acdcc-ce95-4c24-b234-15a5b5ac5b8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Infraestrutura\n",
    "\n",
    "Aproveitando a configuração de cluster da Google Cloud Platform criada para o [projeto Hadoop](https://github.com/pinei/hadoop-precos-combustiveis), incluímos o JupyterLab para trabalhar com o PySpark.\n",
    "\n",
    "![cluster-info](./assets/cluster_info.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbda0644-a8f3-4dd6-9280-de847557dbc8",
   "metadata": {},
   "source": [
    "Em paralelo usamos uma instalação em Raspberry Pi 4 para verificar a viabilidade e desempenho nesse tipo de hardware de baixo custo.\n",
    "\n",
    "- [Running PySpark in JupyterLab on a Raspberry Pi](https://dev.to/pinei/running-pyspark-in-jupyterlab-on-a-raspberry-pi-1293)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a0f7c5-47e4-4d38-855d-63edb15a9667",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Download\n",
    "\n",
    "Os arquivos CSV já estão organizados em um bucket do Google Cloud Storage.\n",
    "\n",
    "> `https://console.cloud.google.com/storage/browser/hadoop-dados-brutos`\n",
    "\n",
    "Optamos por usar uma API Python para trazer os arquivos do storage e trabalhar com eles no cluster ou na máquina local (a depender de como estamos executando este notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef81bc79-511e-4802-9efd-a7a6420ce83e",
   "metadata": {},
   "source": [
    "!pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156074f0-55ab-45fd-8f17-aca6a9b7289a",
   "metadata": {},
   "source": [
    "A API precisa de uma chave para autenticação no serviço.\n",
    "\n",
    "Passo a passo para gerar a chave:\n",
    "- Console do Google Cloud Platform\n",
    "- APIs e Serviços > Credenciais\n",
    "- Criar Conta de Serviço\n",
    "- Criar Chave do tipo JSON\n",
    "\n",
    "> \"É feito o download de um arquivo contendo a chave privada. Armazene o arquivo com segurança porque essa chave não pode ser recuperada em caso de perda.\"\n",
    "\n",
    "Subimos o arquivo JSON para uma pasta de trabalho no cluster:\n",
    "\n",
    "> `/home/aldinei_bastos/work/seventh-abacus-395221-52fc140a5609.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5695d474-4870-41bc-b4b7-462e90b219b7",
   "metadata": {},
   "source": [
    "Fizemos uso da API para acessar o bucket, listar o conteúdo e fazer o download de cada arquivo com extensão CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4b10a38-734b-48bb-abcd-e75e7b8985bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "home_dir = os.environ[\"HOME\"]\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = f'{home_dir}/work/seventh-abacus-395221-52fc140a5609.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7269daf2-2ebf-4d57-85cf-2356a2ca6c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/, 1693781005419814>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2004-01.csv, 1693781239357851>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2004-02.csv, 1693781397853813>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2005-01.csv, 1693781399574637>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2005-02.csv, 1693781397708461>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2006-01.csv, 1693781398243286>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2006-02.csv, 1693781398142808>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2007-01.csv, 1693781395022902>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2007-02.csv, 1693781398311389>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2008-01.csv, 1693781397510159>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2008-02.csv, 1693781398725644>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2009-01.csv, 1693781396903524>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2009-02.csv, 1693781399332015>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2010-01.csv, 1693781396798941>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2010-02.csv, 1693781394972558>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2011-01.csv, 1693781395511546>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2011-02.csv, 1693781398381868>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2012-01.csv, 1693781397727164>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2012-02.csv, 1693781397481250>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2013-01.csv, 1693781395489579>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2013-02.csv, 1693781397154349>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2014-01.csv, 1693781398105663>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2014-02.csv, 1693781395024124>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2015-01.csv, 1693781394795873>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2015-02.csv, 1693781396435277>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2016-01.csv, 1693781398180125>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2016-02.csv, 1693781402222740>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2017-01.csv, 1693781399563611>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2017-02.csv, 1693781399069869>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2018-01.csv, 1693781397978311>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2018-02.csv, 1693781396118663>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2019-01.csv, 1693781397341382>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2019-02.csv, 1693781396351786>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2020-01.csv, 1693781398215772>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2020-02.csv, 1693781395103789>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2021-01.csv, 1693781398387517>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2021-02.csv, 1693781399539579>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2022-01.csv, 1693781399552944>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2022-02.csv, 1693781398198628>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/ca-2023-01.csv, 1693781397325408>,\n",
       " <Blob: hadoop-dados-brutos, anp-combustiveis-automotivos/combustiveis-liquidos-municipios.csv, 1693849151570744>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "# Inicializa o cliente\n",
    "storage_client = storage.Client()\n",
    "\n",
    "# Lista todos os buckets\n",
    "buckets = list(storage_client.list_buckets())\n",
    "\n",
    "# Obtem um bucket específico\n",
    "bucket = storage_client.get_bucket('hadoop-dados-brutos')\n",
    "blobs = list(bucket.list_blobs())\n",
    "\n",
    "display(blobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f77c76c-0116-4856-a375-3f36695b3c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading anp-combustiveis-automotivos/ca-2004-01.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2004-02.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2005-01.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2005-02.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2006-01.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2006-02.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2007-01.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2007-02.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2008-01.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2008-02.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2009-01.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2009-02.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2010-01.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2010-02.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2011-01.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2011-02.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2012-01.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2012-02.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2013-01.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2013-02.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2014-01.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2014-02.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2015-01.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2015-02.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2016-01.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2016-02.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2017-01.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2017-02.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2018-01.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2018-02.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2019-01.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2019-02.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2020-01.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2020-02.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2021-01.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2021-02.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2022-01.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2022-02.csv\n",
      "Downloading anp-combustiveis-automotivos/ca-2023-01.csv\n",
      "Downloading anp-combustiveis-automotivos/combustiveis-liquidos-municipios.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def download_blob(blob):\n",
    "    file_path = f'../work/{blob.name}'\n",
    "        \n",
    "    # Cria a pasta se não existir\n",
    "    dir_name = os.path.dirname(file_path)\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "        \n",
    "    # Download\n",
    "    blob.download_to_filename(file_path)    \n",
    "\n",
    "for blob in blobs:\n",
    "    # Se arquivo CSV\n",
    "    if re.search(r'.csv$', blob.name):\n",
    "        print(f'Downloading {blob.name}')\n",
    "        download_blob(blob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67b7b02-8fc0-4361-8b82-13afeeb96f1b",
   "metadata": {},
   "source": [
    "## Iniciando a base de dados com PySpark\n",
    "\n",
    "Inicializamos uma sessão do Spark com suporte ao Hive para gerenciamento de metadados das bases de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "068ec3f2-5c5c-43a5-9118-ecd60f055ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/12 21:52:03 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = ( SparkSession\n",
    "    .builder\n",
    "    .appName(\"analise-dados\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2da1fc6d-1b77-4501-80d7-648bc523ab67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='Default Hive database', locationUri='hdfs://cluster-anp-spark-m/user/hive/warehouse')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edf99d92-6375-4481-a671-a836a6a322c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|     info_name|          info_value|\n",
      "+--------------+--------------------+\n",
      "|Namespace Name|          precos_anp|\n",
      "|       Comment|Base de dados de ...|\n",
      "|      Location|file:/spark-wareh...|\n",
      "|         Owner|                root|\n",
      "+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_database(name, comment):\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {name} COMMENT '{comment}'\")\n",
    "    return spark.sql(f\"DESCRIBE DATABASE {name}\")\n",
    "\n",
    "metadata = create_database(\"precos_anp\", \"Base de dados de preços de combustíveis fornecidos pela ANP e alguns indicadores de mercado\")\n",
    "metadata.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dad6011-a790-42aa-97b6-87dbc1500481",
   "metadata": {},
   "source": [
    "## Ingestão\n",
    "\n",
    "Definimos o schema para leitura dos arquivos CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08551499-0669-4427-9033-396a1d700a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/12 22:41:12 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (cluster-anp-spark-w-0.c.seventh-abacus-395221.internal executor 2): java.io.FileNotFoundException: \n",
      "File file:/work/anp-combustiveis-automotivos/ca-2004-01.csv does not exist\n",
      "\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate\n",
      "the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\n",
      "recreating the Dataset/DataFrame involved.\n",
      "       \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:660)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:142)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "23/10/12 22:41:15 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o89.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (cluster-anp-spark-w-1.c.seventh-abacus-395221.internal executor 1): java.io.FileNotFoundException: \nFile file:/work/anp-combustiveis-automotivos/ca-2004-01.csv does not exist\n\nIt is possible the underlying files have been updated. You can explicitly invalidate\nthe cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\nrecreating the Dataset/DataFrame involved.\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:660)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:142)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2653)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1189)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2844)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.io.FileNotFoundException: \nFile file:/work/anp-combustiveis-automotivos/ca-2004-01.csv does not exist\n\nIt is possible the underlying files have been updated. You can explicitly invalidate\nthe cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\nrecreating the Dataset/DataFrame involved.\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:660)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:142)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_34747/4121816714.py\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file:///work/anp-combustiveis-automotivos/ca-2004-01.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiLine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\"'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt-BR'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m         \"\"\"\n\u001b[0;32m--> 804\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o89.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (cluster-anp-spark-w-1.c.seventh-abacus-395221.internal executor 1): java.io.FileNotFoundException: \nFile file:/work/anp-combustiveis-automotivos/ca-2004-01.csv does not exist\n\nIt is possible the underlying files have been updated. You can explicitly invalidate\nthe cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\nrecreating the Dataset/DataFrame involved.\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:660)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:142)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2653)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1189)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2844)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.io.FileNotFoundException: \nFile file:/work/anp-combustiveis-automotivos/ca-2004-01.csv does not exist\n\nIt is possible the underlying files have been updated. You can explicitly invalidate\nthe cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\nrecreating the Dataset/DataFrame involved.\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:660)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:142)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, DecimalType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Regiao\", StringType(), True),\n",
    "    StructField(\"Estado\", StringType(), True),\n",
    "    StructField(\"Municipio\", StringType(), True),\n",
    "    StructField(\"Revenda\", StringType(), True),\n",
    "    StructField(\"CNPJ da Revenda\", StringType(), True),\n",
    "    StructField(\"Nome da Rua\", StringType(), True),\n",
    "    StructField(\"Complemento\", StringType(), True),\n",
    "    StructField(\"Bairro\", StringType(), True),\n",
    "    StructField(\"CEP\", StringType(), True),\n",
    "    StructField(\"Produto\", StringType(), True),\n",
    "    StructField(\"Data da Coleta\", DateType(), True),\n",
    "    StructField(\"Valor de Venda\", DecimalType(10, 4), True),\n",
    "    StructField(\"Valor de Compra\", DecimalType(10, 4), True),\n",
    "    StructField(\"Unidade de Medida\", StringType(), True),\n",
    "    StructField(\"Bandeira\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\"file:///work/anp-combustiveis-automotivos/ca-2004-01.csv\", header=True, schema=schema, sep=';', multiLine=True, quote='\"', locale='pt-BR')\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe47da75-7571-4b50-a068-807283c62e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---------+-------+---------------+-----------+-----------+------+---+-------+--------------+--------------+---------------+-----------------+--------+\n",
      "|Regiao|Estado|Municipio|Revenda|CNPJ da Revenda|Nome da Rua|Complemento|Bairro|CEP|Produto|Data da Coleta|Valor de Venda|Valor de Compra|Unidade de Medida|Bandeira|\n",
      "+------+------+---------+-------+---------------+-----------+-----------+------+---+-------+--------------+--------------+---------------+-----------------+--------+\n",
      "+------+------+---------+-------+---------------+-----------+-----------+------+---+-------+--------------+--------------+---------------+-----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
